<!DOCTYPE HTML>
<html lang="en"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-178030306-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
        gtag('config', 'UA-178030306-1');
    </script>

    

  <title>Pankayaraj</title>
  
  <meta name="author" content="Pankayaraj">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="pictures/web_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pankayaraj</name>
              </p>
              <p>Currently, I am a PhD student at the <a href= "https://www.cs.umd.edu/">Department of Computer Science, University of Maryland</a> working with <a href="https://www.cs.umd.edu/people/furongh"> Prof. Furong Huang</a> . Before that I had worked as a research engineer at <a href="https://care-ai.smu.edu.sg/"> CARE AI Lab, Singapore Management University</a> under <a href="http://www.mysmu.edu/faculty/pradeepv/"> Prof. Pradeep Varakantham </a> on constrained reinforcement learning.

              </p>
              <p>
                  
                I have completed my Bachelors of Science in Computer Engineering at <a href="http://www.ce.pdn.ac.lk/"> University of Peradeniya </a>. I have worked at <a href="https://sltc.ac.lk/"> Sri Lanka Technological Campus </a> both as a research Assistant and a research intern under the supervision of <a href="https://eng.pdn.ac.lk/pages/departmentHome/ME/otherpages/staff/Dr%20Maithripala.html"> Prof. D.H.S. Maithripala </a>. During my Undergraduate studies I have won the best final year project research thesis award.
              </p>
              <p style="text-align:center">
                <a href="mailto:p.pankayaraj@gmail.com">Email</a> &nbsp/&nbsp
                  <a href="https://pankayaraj.github.io/reports/Pankayaraj_CV_Short.pdf">Short CV</a> &nbsp/&nbsp
                <a href="https://pankayaraj.github.io/reports/Pankayaraj_CV.pdf">Long CV</a> &nbsp/&nbsp
                <!--- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp --->
                <a href="https://scholar.google.com/citations?user=G_gAOpwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!--- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp --->
                <a href="https://github.com/pankayaraj">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="https://pankayaraj.github.io/"><img style="width:100%;max-width:100%" alt="profile photo" src="pictures/about.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
                <heading><b><u>I. Research Summary</u></b></heading>
              <p>In general my research interests are on Reinforcement Learning(RL). My past experiences in the field of RL span across RLHF, constrainted RL, continual RL, multi agent RL, bayesian RL, bandits. Currently, I am interested working on LLM and RLHF poisoning. Below you can find my past publications and projects. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <heading><b><u>Current Research </u></b></heading>
        </tbody></table>
        
        <br>
         -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <heading><b><u>II. Publications (Selected) </u></b></heading>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <h2><b><u>Large Language Model Alignment and Copyright Generation </u></b></h2>
        </tbody></table>
        
        <br>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            
        <p  style="color:slateblue;font-size:18px" ><b>2024 - 2024: <u><i>LLM Poisoning </i></u></b></p>
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/AdvBDGen.png' width="180"></div>
                    <img src='pictures/AdvBDGen.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="">
                        <papertitle>AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment</papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                        <a>Udari Madhushani Sehwag</a>,
                        <a>Michael-Andrei Panaitescu-Liess</a>,
                        <a href="https://furong-huang.com/">Furong Huang</a>,

                      <br>
                    <br>
                      <b>Published</b>   <em>38 th <b>Neurips Safe Generative AI Workshop 2024</b>  Canada, 2024 </em>
                      <br>
                      <b>Under Review</b>   <em>13 th <b>International Conference of Learning Representations (ICLR)</b>  Singapore, 2025 </em>
                      <br>
                        
                    <a href="https://openreview.net/forum?id=FdQBJu2e4d"> Paper</a>
                      <p> With the growing adoption of reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), the risk of backdoor installation during alignment has increased, leading to unintended and harmful behaviors. Existing backdoor triggers are typically limited to fixed word patterns, making them detectable during data cleaning and easily removable post-poisoning. In this work, we explore the use of prompt-specific paraphrases as backdoor triggers, enhancing their stealth and resistance to removal during LLM alignment. We propose AdvBDGen, an adversarially fortified generative fine-tuning framework that automatically generates prompt-specific backdoors that are effective, stealthy, and transferable across models. AdvBDGen employs a generator-detector pair, fortified by an adversary, to ensure the installability and stealthiness of backdoors. It enables the crafting of complex triggers using as little as 3% of the fine-tuning data. Once installed, these backdoors can jailbreak LLMs during inference, demonstrate improved stability against perturbations compared to traditional constant triggers, and are harder to remove. These properties highlight the greater risks posed by such an adversarially crafted backdoors to LLM alignment.</p>
                </td>   
            </tr>
        
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/PoisonedParrot.png' width="180"></div>
                    <img src='pictures/PoisonedParrot.png' width="180">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="">
                        <papertitle>PoisonedParrot: Subtle Data Poisoning Attacks to Elicit Copyright-Infringing Content from Large Language Models</papertitle>
                      </a>
                      <br>
                      <a>Michael-Andrei Panaitescu-Liess</a>,
                      <strong>Pankayaraj</strong>,
                        <a>Yigitcan Kaya Furong Huang</a>,
                        <a>Bang An</a>,
                        <a>Sicheng Zhu</a>,
                        <a>Aakriti Agrawal</a>,
                        <a href="https://furong-huang.com/">Furong Huang</a>,

                      <br>
                    <br>
                      <b>Published</b>   <em>38 th <b>Neurips Safe Generative AI Workshop 2024</b>  Canada, 2024 </em>
                       <br>
                      <b>Under Review</b>  63rd <em> <b> Annual Meeting of the Association for Computational Linguistics (ACL 2025)</b>  Vienna, 2025 </em>
                      <br>
                    <a href="https://openreview.net/forum?id=ZXgvPANlwe"> Paper</a>
                        
                      <p>As the capabilities of large language models (LLMs) continue to expand, their usage has become increasingly prevalent. However, as reflected in numerous ongoing lawsuits related to LLM-generated content, addressing copyright infringement remains a significant challenge. In this paper, we introduce the first data poisoning attack specifically designed to induce the generation of copyrighted content by an LLM, even when the model has not been directly trained on the specific copyrighted material. We find that a straightforward attack—which integrates small fragments of copyrighted text into the poison samples—is surprisingly effective at priming the models to generate copyrighted content. Moreover, we demonstrate that current defenses are insufficient and largely ineffective against this type of attack, underscoring the need for further exploration of this emerging threat model.</p>
                </td>   
            </tr>
        
          

          
          
        
          
        
          
          
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2023 - 2024: <u><i>Reinforcement Learning with Human Feedback (RLHF) </i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/RLHF.jpeg' width="160"></div>
                    <img src='pictures/RLHF.jpeg' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://github.com/pankayaraj/ICML_2024_RLHFPoisoning">
                        <papertitle>Is poisoning a real threat to LLM alignment? Maybe more so than you think</papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                        <a>   Souradip Chakraborty</a>,
                     <a>   Xiangyu Liu </a>,
                     <a>   Yongyuan Liang </a>,
                     <a href="https://furong-huang.com/">  Furong Huang </a>,

                      <br>
                    <br>
                      
                      <b>Published</b> <em>41 st <b>ICML 2024 Workshop Models of Human Feedback for AI Alignment</b>  Vienna, Austria , 2024 </em>
                    <b> Under Review </b> <em>39 th <b> Annual AAAI Conference on Artificial Intelligence 2025</b>  USA , 2025 </em>
                      <br>
                      <br>
                      
                      <a href="https://github.com/pankayaraj/ICML_2024_RLHFPoisoning"> GitHub</a>
                      
                      <a href="https://arxiv.org/abs/2406.12091"> Paper</a>
                    
                      <p></p>
                      <p>Recent advancements in Reinforcement Learning with Human Feedback (RLHF) have significantly impacted the alignment of Large Language Models (LLMs). The sensitivity of reinforcement learning algorithms such as Proximal Policy
Optimization (PPO) has led to new line work on Direct Policy Optimization
(DPO), which treats RLHF in a supervised learning framework. The increased
practical use of these RLHF methods warrants an analysis of their
vulnerabilities. In this work, we investigate the vulnerabilities of DPO to
poisoning attacks under different scenarios and compare the effectiveness of
preference poisoning, a first of its kind. We comprehensively analyze DPO's
vulnerabilities under different types of attacks, i.e., backdoor and
non-backdoor attacks, and different poisoning methods across a wide array of
language models, i.e., LLama 7B, Mistral 7B, and Gemma 7B. We find that unlike
PPO-based methods, which, when it comes to backdoor attacks, require at least
4% of the data to be poisoned to elicit harmful behavior, we exploit the true
vulnerabilities of DPO more simply so we can poison the model with only as much
as 0.5% of the data. We further investigate the potential reasons behind the
vulnerability and how well this vulnerability translates into backdoor vs
non-backdoor attacks. </p>
                </td>   
            </tr>
        
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2023 - 2024: <u><i>LLM Watermarking and Copyright Generation</i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/Copyright.png' width="160"></div>
                    <img src='pictures/Copyright.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://github.com/pankayaraj/LLM_Next_Word_Prediction">
                        <papertitle>Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?</papertitle>
                      </a>
                      <br>
                      
                      
                        <a>   Michael-Andrei Panaitescu-Lies</a>,
                     <a>  Zora Che </a>,
                     <a>   Bang An </a>,
                    <a> Yuancheng Xu</a>
                    <strong>Pankayaraj</strong>,
                    <a> Souradip Chakraborty </a>
                    <a> Sicheng Zhu</a>
                    <a>Tom Goldstein</a>
                     <a href="https://furong-huang.com/">  Furong Huang </a>,

                    <br>
                    <br>
                    
                    <b> Published </b> <em>41 st <b>ICML 2024 Workshop NextGenAISafety </b>  Vienna, Austria , 2024 </em> 
                    <br>
                    <b> Published </b> <em>3 rd<b> NeurIPS 2024 Workshop AdvMLFrontiers </b> Vancover, Canada , 2024 </em>
                    <br>
                    <b> Under Review </b> <em>39 th <b> Annual AAAI Conference on Artificial Intelligence 2025</b>  USA , 2025 </em>  
                    <br>
                    
                
                    
                                    
                      
                      <a href="https://github.com/pankayaraj/LLM_Next_Word_Prediction"> GitHub</a>
                      
                      <a href=""> Paper</a>
                    
                      <p></p>
                      <p>Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this paper, we first investigate the effectiveness of watermarking LLMs as a deterrent against the generation of copyrighted texts. Through theoretical analysis and empirical evaluation, we demonstrate that incorporating watermarks into LLMs significantly reduces the likelihood of generating copyrighted content, thereby addressing a critical concern in the deployment of LLMs. Additionally, we explore the impact of watermarking on Membership Inference Attacks (MIAs), which aim to discern whether a sample was part of the pretraining dataset. Surprisingly, we find that watermarking adversely affects the success rate of MIAs, complicating the task of detecting copyrighted text in the pretraining dataset. Finally, we propose an adaptive technique to improve the success rate of a recent MIA under watermarking. Our findings underscore the importance of developing adaptive methods to study critical problems in LLMs with potential legal implications. </p>
                </td>   
            </tr>
        </tbody></table>    
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>         
        <h2><b><u>Reinforcement Learning </u></b></h2>
        </tbody></table>
          
        
        
        
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2022 - 2022: <u><i>Constrained Reinforcement Learning</i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/Four_Roooms.png' width="160"></div>
                    <img src='pictures/Four_Roooms.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://github.com/pankayaraj/AAAI_2023_Hierarchical-constrained-RL">
                        <papertitle>Constrained Reinforcement Learning in Hard Exploration Problems with Hierarchies</papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                        <a href="http://www.mysmu.edu/faculty/pradeepv/">  Pradeep Varakantham </a>,
                    <br>
                      <br>
                         <b> Published </b> <em>37th <b>AAAI Conference on Artificial Intelligence</b>  Washington, D.C. USA , 2022 <b>Acceptance Rate: 19.6% </b></em>
                      <br>
                      
                      <a href="https://github.com/pankayaraj/AAAI_2023_Hierarchical-constrained-RL"> GitHub</a>
                      /
                      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26757"> Paper</a>
                    
                      <p></p>
                      <p>In this work, we propose a method to incorporate and satisfy constraints at every time step in a hierarchical reinforcement learning (HRL) framework. In particular, we propose a way to incorporate backward value functions into an options-based HRL framework. This incorporation depends on the fact that there exists a steady distribution in the HRL framework. To this end under some assumptions, we prove the existence of such a stationary distribution for the markov decision process at every level of the hierarchy. Furthermore, empirically we show the importance of our proposal in terms of efficient exploration as normally the exploration gets curtailed as constraint satisfaction becomes a focal point agent </p>
                </td>   
            </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2021 - 2021: <u><i>Continual Reinforcement Learning</i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/cur_cl_rl.PNG' width="160"></div>
                    <img src='pictures/cur_cl_rl.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://github.com/pankayaraj/Cognitive_Computation-2023_Continual-Learning-With-Curiosity">
                        <papertitle>Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning</papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                        <a href="https://sites.google.com/view/nataliadiaz">  Natalia Díaz-Rodríguez</a>,
                        <a href="https://www.tecnalia.com/"> Javier Del Ser</a>,

                      <br>
                    <br>
                        <b> Published </b> <em> <b>Cognitive Computation journal. </b>Accepted in 2023. <b> Impact Factor: 5.4 </b></em>
                      <br>
                      
                      <a href="https://github.com/pankayaraj/Cognitive_Computation-2023_Continual-Learning-With-Curiosity"> GitHub</a>
                      /
                      <a href="https://link.springer.com/article/10.1007/s12559-023-10213-9"> Paper</a>
                    
                      <p></p>
                      <p>In this work, we investigate the means of using curiosity on replay buffers to improve offline multi-task continual reinforcement learning when tasks, which are defined by the non-stationarity in the environment, are non labeled and \emph{not evenly exposed to the learner in time </p>
                </td>   
            </tr>
        </tbody></table>
          
        
        
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2020 - 2020: <u><i>Multi Agent Reinforcement Learning</i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/marl_1.PNG' width="160"></div>
                    <img src='pictures/marl_1.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://www.researchgate.net/publication/351599797_Temporally_Aware_Multi-Agent_Reinforcement_Learning_in_Sparsely_Connected_Cooprative_Enviornments">
                        <papertitle>Temporally Aware Multi-Agent Reinforcement Learning  in Sparsely Connected Cooperative Environments </papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                      <a href="https://www.linkedin.com/in/yuvini-sumanasekera/?originalSubdomain=lk">Yuvini Sumanasekera</a>,
                      <a href="https://www.linkedin.com/in/chandima-samarasinghe-2235b298/?originalSubdomain=lk">Chandima Samarasinghe</a>,
                      <a href="http://www.ce.pdn.ac.lk/academic-staff/dhammika-elkaduwe/">Dhammika Elkaduwe</a>,
                      <a href="http://www.ce.pdn.ac.lk/upul/">Upul Jayasinghe </a>,
                      <a href="https://eng.pdn.ac.lk/pages/departmentHome/ME/otherpages/staff/Dr%20Maithripala.html">D.H.S Maithripala</a>,
                      
                      <br>                
                      <br>
                        <b> Published </b> <em> <b>ESCaPe (Symposium)</b>, Sri Lanka</em>, 2020. <b>Best Paper Award</b>
                      <br>

                      <a href="https://www.researchgate.net/publication/351599797_Temporally_Aware_Multi-Agent_Reinforcement_Learning_in_Sparsely_Connected_Cooprative_Enviornments">Researchgate</a>
                      /
                      <a href="https://github.com/pankayaraj/Temporal-Attention-Based-MARL"> GitHub</a>

                      <p></p>
                      <p>In this work,  we propose a model which exploitsthe inherent graph-like structure of multi-agent networks to facilitate the learning of more robustbehaviour strategies by capturing the spatial dependencies and temporal dynamics of the underlying graph.</p>
                </td>   
            </tr>
            
        </tbody></table>
          
        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2019 - 2020: <u><i>Reinforcement Learning Based Quadcopter Control</i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/quad_review.PNG' width="160"></div>
                    <img src='pictures/quad_review.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://www.researchgate.net/publication/352164771_A_Review_on_Reinforcement_Learning_Based_Autonomous_Quadcopter_Control">
                        <papertitle>A Review on Reinforcement Learning Based Autonomous Quadcopter Control </papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                      <a href="https://www.linkedin.com/in/chandima-samarasinghe-2235b298/?originalSubdomain=lk">Chandima Samarasinghe</a>,
                      <a href="https://www.linkedin.com/in/yuvini-sumanasekera/?originalSubdomain=lk">Yuvini Sumanasekera</a>,

                      <br>
                                    <em> Pre-Print</em>
                      <br>

                      <a href="https://www.researchgate.net/publication/352164771_A_Review_on_Reinforcement_Learning_Based_Autonomous_Quadcopter_Control">Researchgate</a>
                      
                     

                      <p></p>
                      <p>In recent years, extensive research has been carriedout  in  the  field  of  autonomous  aerial  vehicle  control,  motivatedby the rapid advancements in Machine Learning (ML). In partic-ular, Reinforcement Learning (RL) has gained immense interestin   developing  control   algorithms.  In   this   work,  we   examinekey  control  problems  related  to  the  operation  of  quadcoptersautonomously  and  analyze  how  past  efforts  have  applied  RLtechniques to approach these problems.</p>
                </td>   
            </tr>
            
        </tbody></table>
         -->
          
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <h2><b><u> Multi Arm Bandits  </u></b></h2>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <p  style="color:slateblue;font-size:18px" ><b>2019 - 2020: <u><i>Multi Agent Multi Arm Bandit Research</i></u></b></p>
        
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/cdc_1.PNG' width="160"></div>
                    <img src='pictures/cdc_1.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/2003.12968">
                        <papertitle>A Decentralized Policy with Logarithmic Regret for a Class of Multi-Agent Multi-Armed Bandit Problems with Option Unavailability Constraints and Stochastic Communication Protocols </papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                      <a href="https://www.nsf.gov/staff/staff_bio.jsp?lan=jberg&org=CMMI&from_org=CMMI"> J. M. Berg</a>,
                      <a href="https://eng.pdn.ac.lk/pages/departmentHome/ME/otherpages/staff/Dr%20Maithripala.html">D.H.S Maithripala</a>,
                      
                      <br>
                      <br>
                       <b> Published </b> <em>59th <b>IEEE Conference on Decision and Control(IEEE CDC)</b>, Jeju Island, Republic of Korea 2020 <b>Acceptance Rate: 52.7% </b></em>
                      <br>

                      <a href="https://arxiv.org/abs/2003.12968">arXiv</a>
                      /
                      <a href="https://ieeexplore.ieee.org/document/9304002">IEEE</a>
                      

                      <p></p>
                      <p>This paper considers a multi-armed bandit (MAB) problem in which multiple mobile agents receive rewards by sampling from a collection of spatially dispersed bandits. The goal is to formulate a decentralized policy for each agent, in order to maximize the total cumulative reward over all agents, subject to option availability and inter-agent communication constraints.</p>
                </td>   
            </tr>
            
            
           <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/ecc_1.PNG' width="160"></div>
                    <img src='pictures/ecc_1.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/abs/1910.02635">
                        <papertitle>A Decentralized Communication Policy for Multi Agent Multi Armed Bandit Problems </papertitle>
                      </a>
                      <br>
                      
                      <strong>Pankayaraj</strong>,
                      <a href="https://eng.pdn.ac.lk/pages/departmentHome/ME/otherpages/staff/Dr%20Maithripala.html">D.H.S Maithripala</a>,
                      
                      <br>
                      <br>
                        <b> Published </b> <em> <b>European Control Conference(ECC)</b>, Saint Petersburg, Russia 2020 <b>Acceptance Rate: 58% </b></em>, 
                      <br>

                      <a href="https://arxiv.org/abs/1910.02635">arXiv</a>
                      /
                      <a href="https://ieeexplore.ieee.org/document/9143811">IEEE</a>
                      /
                      <a href="https://github.com/pankayaraj/MAMAB_ECC_20"> GitHub</a>

                      <p></p>
                      <p>This paper proposes a novel policy for a group of agents to, individually as well as collectively, solve a multi armed bandit (MAB) problem. The policy relies solely on the information that an agent has obtained through sampling of the options on its own and through communication with neighbors.</p>
                </td>   
            </tr>
        </tbody></table>
          
          <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <h2><b><u> Computer Vision Based Applications  </u></b></h2>
        </tbody></table>
          
        
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <p  style="color:slateblue;font-size:18px" ><b>2018 - 2019: <u><i>Sleep Apnea Detection</i></u></b></p>
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/sleep_apnea.png' width="160"></div>
                    <img src='pictures/sleep_apnea.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://arxiv.org/pdf/1910.04725.pdf">
                        <papertitle>Non-contact Infant Sleep Apnea Detection</papertitle>
                      </a>
                      <br>
                      <a href="https://gihan.me/">Gihan Jayatilaka</a>,
                      <a href="https://harshana95.github.io/">Harshana Weligampola</a>,
                      <a href="https://suren3141.github.io/">Suren Sritharan</a>,
                      <strong>Pankayaraj</strong>,
                      <a href="http://www.ce.pdn.ac.lk/academic-staff/roshan-g-ragel/">Roshan Ragel</a>,
                      <a href="http://www.ce.pdn.ac.lk/academic-staff/isuru-nawinne/">Isuru Nawinne</a>,
                      <br>
                      <br>
                                    <em><b>ICIIS</b>, Sri Lanka</em>, 2019
                      <br>

                      <a href="https://arxiv.org/pdf/1910.04725.pdf">arXiv</a>
                      /
                      <a href="https://ieeexplore.ieee.org/document/9063269">IEEE</a>
                      /
                      <a href="https://github.com/pankayaraj/Sleep_Apnea_Detection-1"> GitHub</a>

                      <p></p>
                      <p>We propose a non invasive solution for this problem based on video processing. The infant is observed by a video camera which is connected to a single board computer (Raspberry pi) which analyzes the video feed to diagnose breathing anomalies. The camera is turned to a proper orientation for the observation using a robotic arm.</p>
                </td>   
            </tr>
        </tbody></table> 
          
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
                <heading><b><u>III. Academic Volunteering</u></b></heading>
            </td>
          </tr>
        </tbody></table>
          
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="pictures/review/journal_pic.PNG"></td>
            <td width="75%" valign="center">
              <a href="https://publons.com/researcher/3858300/pankayaraj-pathmanathan/">Peer Reviewer : Journal IEEE Transactions on Communications</a> [Impact Factor: 5.69(2018)]    
            </td>
          </tr>		
					
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td>
                <heading><b><u>IV. Projects (These are my undergrad projects)</u></b></heading>
            </td>
          </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/undergraduate/airsim.png' width="160"></div>
                    <img src='pictures/undergraduate/airsim.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Quadcopter_control_RL.pdf">
                        <papertitle>RL based Quadcopter Control</papertitle>
                      </a>
                      <br>
                    
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Quadcopter_control_RL.pdf">Report</a>

                      <p></p>
                      <p>In recent years, extensive research has been carried out in the field of autonomous aerial vehicle control, motivated by the rapid advancements in Machine Learning (ML). In particular, Reinforcement Learning (RL) has gained immense interest in developing control algorithms given its ability to learn useful behavior by dynamically interacting with the environment, without the need for an explicit teacher. In this work, we examine the use of RL methods on vision-based quadcopter control in both single-agent and multi-agent simulated environments. Specifically, the DQN algorithm was investigated in the single-agent setting and the MADDPG algorithm in the multi-agent setting. The control task in each of these settings was to navigate through the environment by avoiding obstacles to reach the specified goals. Thus, each of the aforementioned algorithms were evaluated on their ability to perform this control task.</p>
                </td>   
            </tr>
            
            
            <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/undergraduate/MARL_S.PNG' width="160"></div>
                    <img src='pictures/undergraduate/MARL_S.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://pankayaraj.github.io/reports/undergraduate/MARL_Sparse.pdf">
                        <papertitle>
Multi Agent Reinformcent Learning with Sparse Communication</papertitle>
                      </a>
                      <br>
                    
                      <a href="https://pankayaraj.github.io/reports/undergraduate/MARL_Sparse.pdf">Report</a>
                      \
                     <a href="https://github.com/pankayaraj/Temporal-Attention-Based-MARL"> GitHub</a>

                      <p></p>
                      <p>In recent years, the consensus among adaptive agents within multi-agent systems (MAS) has been an emerging area of research in the field of autonomous control. Reinforcement Learning (RL) has gained immense interest in this line of work as it aims to learn optimal cooperative policies through trial and error by dynamically interacting with the environment. However, in practice, connectivity within the multi-agent network may be sparse and the agents are often subjected to partial observability. This can result in the learning of sub-optimal policies. In this work, we consider the problem of learning optimal policies in cooperative multi-agent environments in the face of partial observability and sparse connectivity. The proposed model exploits the inherent graph-like structure of multi-agent systems. Graph Neural Networks (GNNs) are utilized to extract spatial dependencies and temporal dynamics of the underlying graph. Such spatio-temporal information is exploited to generate better state representations so as to facilitate the learning of more robust policies. This model builds on the previously explored spatial modelling in MARL.</p>
                </td>   
            </tr>
            
        
          
        <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/undergraduate/recommendation_system.PNG' width="160"></div>
                    <img src='pictures/undergraduate/recommendation_system.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Recommendation_using_Bayesian_RL.pdf">
                        <papertitle>Bayesian RL based Recommendation System</papertitle>
                      </a>
                      <br>
                    
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Recommendation_using_Bayesian_RL.pdf">Report</a>
                      \
                     <a href="https://github.com/pankayaraj/sitnshop/tree/master/GPSARSA"> GitHub</a>

                      <p></p>
                      <p>When it comes to user customization it is essential to capture users preferences in an optimal manner so that the user can be served based on his past preferences. The concept behind this work is to formulate an a methodology for an online advertising shop to customize it’s advertisement presentation using the existing algorithms in the literature. The task of the algorithm is to find the next shop to suggest for the user on his time line based on his past preferences. Users preferences will be captured by the ratings he give for a shop when it is shown in his time line and by the fact weather he marks some shop as visited. Most part of the final suggested algorithm follows the 2003 paper named Bayes Meets Bellman: The Gaussian Process Approach to Temporal Difference Learning</p>
                </td>   
        </tr>
            
            
        <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/undergraduate/bayesian_optimization.PNG' width="160"></div>
                    <img src='pictures/undergraduate/bayesian_optimization.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Bayseian_optimization.pdf">
                        <papertitle>Python based Bayesian Optimization library</papertitle>
                      </a>
                      <br>
                    
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Bayseian_optimization.pdf">Report</a>
                      \
                      <a href="https://github.com/pankayaraj/Multi-Arm-Bandit-Library"> GitHub</a>
                      \
                      <a href="https://pypi.python.org/pypi/mabandit/1.3"> PyPi </a>

                      <p></p>
                      <p>In probability theory multi arm bandit problem or N-arm bandit problem is a problem in which a gambler at a row machine have to choose which machine to play and how many time to play it given a limited number of turns to choose. When chosen a machine would give a particular amount of reward which is either deterministic or probabilistic. Thus to accumulate an optimal amount of reward the gambler should choose a an optimal solution without knowing the reward structure behind the machine.
                      As the problem moved away from the discrete arms got extended as a continuous variable with a K dimension the problem got extended as continuous bandit problem. Since the no of bandits became infinite to reduce the complexity the problem was formulated with deterministic rewards where the rewards of each arm were considered as a correlated function. As the scope of these problems narrowed down to the bayesian thinking they were named as bayesian optimization. They can be considered as a problem where we are supposed to optimize a function with certain bounds with as few samples as possible.In this work we provide a python based library for the above mentioned bayesian optimization problem</p>
                </td>   
        </tr>
            
       
            
        <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/undergraduate/sitnshop.PNG' width="160"></div>
                    <img src='pictures/undergraduate/sitnshop.PNG' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://pankayaraj.github.io/reports/undergraduate/SitNShop.pdf">
                        <papertitle>SitNShop</papertitle>
                      </a>
                      <br>
                    
                      <a href="https://pankayaraj.github.io/reports/undergraduate/SitNShop.pdf">Report</a>
                      \
                     <a href="https://github.com/pankayaraj/sitnshop"> GitHub</a>

                      
                      <p>The concept behind this project is to design and implement a web page to connect the local customers with the local shop owners by building a platform for advertisements. This project is build on the basis of providing an interactive interface for both users and shop owners with the ability to convey the information about them as much as possible while focusing also on the development of a capable algorithm to capture the preference of the customer dynamically.</p>
                </td>   
        </tr>
            
        
        <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='nerfactor_image'>
                      <img src='pictures/undergraduate/sleep_apnea.jpg' width="160"></div>
                    <img src='pictures/undergraduate/sleep_apnea.jpg' width="160">
                  </div>
                  <script type="text/javascript">
                    function nerfactor_start() {
                      document.getElementById('nerfactor_image').style.opacity = "1";
                    }

                    function nerfactor_stop() {
                      document.getElementById('nerfactor_image').style.opacity = "0";
                    }
                    nerfactor_stop()
                  </script>
                </td>

                <td style="padding:20px;width:75%;vertical-align:middle">
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Seep_apnea_detection_report.pdf">
                        <papertitle>Non Contact Sleep Apnea Detection</papertitle>
                      </a>
                      <br>
                    
                      <a href="https://pankayaraj.github.io/reports/undergraduate/Seep_apnea_detection_report.pdf">Report</a>
                      \
                     <a href="https://github.com/pankayaraj/Sleep_Apnea_Detection-1"> GitHub</a>

                      <p></p>
                      <p>Sleep Apnea is a serious disorder caused by the interruption of breathing during sleep. This can cause the people to stop breathing for several time even hundreds if not treated properly. It can affect people of any age. But when the babies are affected with the condition they tend to not get up and keep on sleeping which may risk their lives. We propose a non invasive solution for this problem based on video processing. The infant is observed by a video camera which is connected to a single board computer (Raspberry pi) which analyzes the video feed to diagnose breathing anomalies. The camera is turned to a proper orientation for the observation using a robotic arm.</p>
                </td>   
        </tr>
            
        </tbody></table>
        
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <heading><b><u>V. References</u></b> </heading>
          <tr>
            <td style="padding:00px">
              
              <p>
              <a 
              href= https://furong-huang.com/ >1. Prof. Furong Huang</a> <br>
              Assistant Professor <br>
              
              University of Maryland, Department of Computer Science<br>
              <a href="mailto:furongh@umd.edu">furongh@umd.edu</a>
              </p> 
              
                
              <p>
              <a 
              href= http://www.mysmu.edu/faculty/pradeepv/ >1. Prof. Pradeep Varakantham</a> <br>
              Lee Kuan Yew Fellow <br>
              Professor of Computer Science <br>
              School of Computing and Information Systems, Singapore Management University<br>
              <a href="mailto:pradeepv@smu.edu.sg ">pradeepv@smu.edu.sg</a>
              </p> 
                
                
              <p>
              <a href="https://eng.pdn.ac.lk/pages/departmentHome/ME/otherpages/staff/Dr%20Maithripala.html">2. Dr. D.H.S Maithripala </a> <br>
              Senior Lecturer <br>
              Department of Mechanical Engineering, University of Peradeniya, Sri Lanka. <br>
              <a href="mailto:smaithri@pdn.ac.lk">smaithri@pdn.ac.lk</a>
                
              </p>
                
            
                <!-- 
              
              <p>
              <a href="http://eng.pdn.ac.lk/old/engmath/staff/Dr-rathnamali.html">4. Dr. G. W. R. M. R. Palamakumbura </a> <br>
              Senior Lecturer <br>
              Department of Department of Engineering Mathematics, University of Peradeniya, Sri Lanka. <br>
              <a href="mailto:rpalam@pdn.ac.lk">rpalam@pdn.ac.lk</a>
                
              </p>
                
              <p>
              <a href="http://www.ce.pdn.ac.lk/academic-staff/isuru-nawinne/">5. Dr. Isuru Nawinne </a> <br>
              Senior Lecturer <br>
              Department of Computer Engineering, University of Peradeniya, Sri Lanka. <br>
              <a href="mailto:isurunawinne@eng.pdn.ac.lk">isurunawinne@eng.pdn.ac.lk</a>
                
              </p>
              -->
            </td>
          </tr>
        </tbody></table>
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:80px">
              <br>
              <p style="text-align:right;font-size:small;">
              Credits: <a href="https://jonbarron.info/"> <b>Jon Barron</b></a> <br>
              Last Updated: <b>28-Dec-2021</b>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
