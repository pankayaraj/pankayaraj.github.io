
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
    
  <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-178030306-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
        gtag('config', 'UA-178030306-1');
    </script>

  <title>Pankayaraj</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/web_icon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: iPortfolio - v3.3.0
  * Template URL: https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>
    
    
<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <i class="bi bi-list mobile-nav-toggle d-xl-none"></i>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/profile_pic.jpg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Pankayaraj</a></h1>
        <div class="social-links mt-3 text-center">
          
          <a href="https://github.com/punk95" class="GitHub"><i class="bx bxl-github"></i></a>
          <a href="https://www.linkedin.com/in/pankayaraj-pathmanathan-259926119/" class="linkedin"><i class="bx bxl-linkedin"></i></a>  
          <a href="https://www.instagram.com/pankayaraj" class="Instagram"><i class="bx bxl-instagram"></i></a>
          
          
        </div>
      </div>

      <nav id="navbar" class="nav-menu navbar">
        <ul>
          <!--<li><a href="#hero" class="nav-link scrollto active"><i class="bx bx-home"></i> <span>Home</span></a></li>  -->
          <li><a href="https://punk95.github.io/" class="nav-link scrollto"><i class="bx bx-user"></i> <span>Homepage</span></a></li>
          
        </ul>
      </nav><!-- .nav-menu -->
    </div>
  </header><!-- End Header -->

<section id="faq" class="faq">
         
         
         <div class="container">
             
             <div class="section-title">
                 <h2>Research Questions and Potential Directions</h2>
                 <p>I am using this page to list the research question in certain areas and what I believe is the way to tackle them. </p>
                 <p><strong>Select the subsection</strong> your are intrested in and then <strong>click on the question </strong> that interests you to expand </p>
                 <a id="v"></a>
             </div>
             
             
             <div class="row" data-aos="fade-up">
                <div class="col-lg-12 d-flex justify-content-center">
                    <ul id="faq-flters">
                    <li id= "default" data-filter=".filter-safeRL">Safe RL</li>
                    <li data-filter=".filter-HRC"> Human Robot Collaboration</li>
                    <li data-filter=".filter-MARL">Multi- Agent RL</li>
                   
                    </ul>
                </div>
            </div>
            
             
            
            
            <div class="row faq-container" data-aos="fade-up" data-aos-delay="100">  
                         
                 <div class="col-lg-12 faq-item filter-safeRL ">
                     
                    <h3>Safe Reinforcement Learning</h3>
                    
                     <ol>
                        
                          <li>  
                             <button type="button" class="collapsible">Implementing a hard/ soft constraint on actions values that are selected by agent.</button>
                             <div class="content">
                                 <h5>Motivation</h5>
                                 
                                <p>When it comes to constrained reinforcement learning I do primarily see the problem as something that needs to be seen in two perspectives. One is the need to maintain the constraints imposed at all times (for instance scenarios where certain limitations should be observed in order to avoid damage to the robot) while the other being scenarios where the bounds need to be observed only on the testing phase. When it concerns the latter I believe that any kind of structure that imposes constraint as a subordinate reward would suffice. But when it comes to the former the computational cost of bringing the non optimal feature (action in most cases) to an optimum at every stage and computation of the gradient with regards to that process seems to be higher. (In most cases involves finding the inverse of larger matrices etc)
                                I do think that the computational cost in the former is unavoidable if we are to never breach the constraints on training even. But I do have my eyes on ways to reduce the computational cost in the latter which I will elaborate further below. </p>
                        
                                <p>
                                If the constraints are convex in the function space (neural network in our case) I think we can play a two player lagrangian dual primal game and achieve the optimal constraint (achieve a pure Nash equilibrium since the minimax theorem holds) with a first order process who’s gradient can be computed similar to what’s done in model agnostic meta learning. What’s more interesting is the non convex setting where the constraints are not only non convex but also the gradients are not available at all times (due to discontinuity). I am also excited about the prospects of exploiting the recent developments on proxy constraints  in the same setting as above. </p>
                             </div>
                          </li> 
                         
                         
                         
                         <li>  
                             <button type="button" class="collapsible">Safe RL via the modification of optimization criterion when due to stochasticity in the environment.</button>
                             <div class="content">
                                 <h5>Background</h5>
                                 
                                 <p>When there is variability in the policy then there is always a possibility of a certain agent at a certain time getting to the sub optimal trajectory which can be a harmful one. Thus a generally used option with this regard is to explicitly make sure that return of the least performing trajectory of the policy is maximized. This can ensure that the least performing trajectory itself is not in a low return (which is assumed to be damaging) is not as bad. This does come with its own issues as this objective can be too pessimistic and the overall objective of the policy can be suboptimal. There are some alternatives to this which either only selects the low return trajectory that passes the ɑ value threshold thus the algorithm is not that pessimistic or else have dual objective (regular one and the earlier mentioned min max criterion) via a floating parameter between 0 and 1. </p>

                                <p></p>
                             </div>
                          </li> 
    
                    </ol>
                     
                     
                     
                     
                     <h4></h4>
                     
                 </div>

                 <div class="col-lg-12 faq-item filter-HRC ">
                     
                    <h3>Human Robot Collaboration</h3>
                    
                    <ol>
                        
                         <li>  
                             
                             <button type="button" class="collapsible">Role of Heirarchy in Imitation </button>
                             <div class="content">
                                <p>When it comes to RL in practice ( in human collaboration etc)  I think we should take inspiration on how babies learn. For example when a baby who wants a sweet jar that's on a higher elevation and wants to access it but don't have the necessary skills for it, for example, use a ladder (An example used in "Computational theories of curiosity-driven learning" by Pierre-Yves-Oudeyer on a different context) the baby would see a human behaving in a different context and learns his sub goals (for example, a human using a chair to get something that's higher) and then try to explore getting to the jar by exploring in lines with the sub goals learned from the human (using a chair to climb). I think this is a great context to explore human robot collaboration. Particularly, as an inverse hierarchical reinforcement framework. This motivation and  expansion of the lines is what motivated my interest in human robot learning. Also when we talk about either inverse learning or behavioural cloning in a hierarchical framework it alleviates the complexity issue which for the most part acts as a barrier in these fields.</p>

                                <p>When it comes to robots teaching non expert agents, I also do believe that hierarchical inverse learning is something that we need to explore. Because, when it comes to robots teaching humans I think scenario generation based on curriculums is essential ( in order to teach humans). While GAN based task generation is something we can pursue with this regard, unless hierarchies are formed the generation process itself can become increasingly complex. 

                             </div>
                         </li> 
                    <!-- 
                         <li>  
                             <button type="button" class="collapsible">Question 2</button>
                             <div class="content">
                                <p>Lorem ipsum...</p>
                             </div>
                         </li> 
                    -->
                        
                    
                    
                    </ol>

                 </div>
                
                
                 <div class="col-lg-12 faq-item filter-MARL ">
                     
                    <h3>Multi Agent Reinforcement Learning</h3>
                    
                    <ol>
                            
                         <li>  
                             <button type="button" class="collapsible">Thoughts on Large scale / Swam Multi Agent Reinforcement Learning</button>
                             <div class="content">
                                <p>When it comes to multi-agent reinforcement learning, in order to get large scale employable solutions I think we need to explicitly identify and attribute cliques of agents for certain changes. This can take away a lot of burden away from the value function and allow us to learn more complex planning. Also explicit credit assignment to certain cliques  can help immensely in the case of adversarial agents. Another problem that may arise in large scale multi agent RL is sparsity in communication. To this regard I am excited in leveraging the recent developments in the hopfield networks (in the continuous domain) and try to see whether they can play a role in estimating the clique even when information with regards to certain members of the clique is not available for instance due to them being outside of the agent's point of view. </p>
                             </div>
                         </li> 
                        
                        
                    
                    
                    </ol>

                        
           </div>

             
        </div>

    </section>
      
    
<a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

         
</body>
</html>
